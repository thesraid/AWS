#!/bin/bash

# joriordan
# Prints out the usage of the scipt
function usage
{
    echo "usage: labFinish [-h]
				      --region [-r] REGION
				      --account_name [-a] ACCOUNT_NAME
				      Multiple accounts can be specified in quotes"
}

if [ $# -eq 0 ]
  then
    usage
    exit
fi


# Create a variable to store the account name & region
accName=""

# Reads the input switches and assigns the Account Name to the accName 
while [ "$1" != "" ]; do
    case $1 in
        -r | --region )   	shift
                                singleRegion=$1
                                ;;
        -a | --account_name )   shift
                                accNameArray=($1)
                                ;;
        -h | --help )           usage
                                exit
                                ;;
    esac
    shift
done


# Made the script a function so that it can be called a few times in case of an error. This allows the script to retry on failure. 
function removeAccount
{

echo "Warming up."
sleep $[ ( $RANDOM % 59 )  + 1 ]s

accName=$1
echo "accName is $accName"

# Find the accountID for the entered account.
accID=$(aws organizations list-accounts-for-parent --parent-id ou-yus6-zcr8xf63 --query 'Accounts[?Name==`'$accName'`].Id' --output text)
if [ "$accID" = "" ] 
then
   printf "Invalid account\n"
   exit
fi
printf "The account ID for $accName is $accID\n"

# The profile is what aws cli uses to identify the account. For simplicity we will set this to the account name
# We will also greate a user and group with the same name
profile=$accName
userName=$accName
groupName=$accName

#Get a token from AWS so we can talk to the subOrg
role="OrganizationAccountAccessRole"
# Get a ticket to allow us to use the role that was created in the sub Orgs during their creation
expiry=$(aws sts assume-role --role-arn arn:aws:iam::$accID:role/$role --query 'Credentials.Expiration' --role-session-name $profile)
if [ $? -ne 0 ]
then
  printf "Error occured connecting to the account\n"
  sendEmail.py -r USM-Anywhere-Training@alienvault.com -s "$profile Failed to Finish" -b "Do not reply to this e-mail<br/>The labFinish script tried to finish $profile and failed.<br/>The AWS Account $profile was not accessible.<br/>Manual intervention is required<br/>Try running labConnect on it's own with no arguments"
  exit 1
fi

printf "Connection valid to $accName until $expiry\n"

# Set the region to us-east-1 to start with. 
aws configure set region us-east-1 --profile $profile
if [ $? -ne 0 ]
then
  printf "Error occured connecting to the account\n"
  sendEmail.py -r USM-Anywhere-Training@alienvault.com -s "$profile Failed to Finish" -b "Do not reply to this e-mail<br/>The labFinish script tried to finish $profile and failed.<br/>The AWS Account $profile was not accessible.<br/>Manual intervention is required<br/>Try running labConnect on it's own with no arguments"
  exit 1
fi

# Assume the role created in the Sub Orgs for the following commands
aws configure set role_arn arn:aws:iam::$accID:role/$role --profile $profile
if [ $? -ne 0 ]
then
  printf "Error occured connecting to the account\n"
  sendEmail.py -r USM-Anywhere-Training@alienvault.com -s "$profile Failed to Finish" -b "Do not reply to this e-mail<br/>The labFinish script tried to finish $profile and failed.<br/>The AWS Account $profile was not accessible.<br/>Manual intervention is required<br/>Try running labConnect on it's own with no arguments"
  exit 1
fi

# Connect to the Sub Org
aws configure set source_profile default --profile $profile
if [ $? -ne 0 ]
then
  printf "Error occured connecting to the account\n"
  sendEmail.py -r USM-Anywhere-Training@alienvault.com -s "$profile Failed to Finish" -b "Do not reply to this e-mail<br/>The labFinish script tried to finish $profile and failed.<br/>The AWS Account $profile was not accessible.<br/>Manual intervention is required<br/>Try running labConnect on it's own with no arguments"
  exit 1
fi

printf "\n"
USERGROUPS=($(aws iam list-groups --output text --query 'Groups[*].GroupName' --profile $profile))
for group in "${USERGROUPS[@]}"
do
   printf "\nThis is group: $group\n"
   GROUPPOLICIES=($(aws iam list-group-policies --output text --query 'PolicyNames[*]' --group-name $group --profile $profile))
   for policy in "${GROUPPOLICIES[@]}"
   do
      printf "Deleting policy: $policy\n"
      aws iam delete-group-policy --group-name $group --policy-name $policy --profile $profile
   done
   USERS=($(aws iam get-group --group-name $group --query 'Users[*].UserName' --output text --profile $profile))
   for user in "${USERS[@]}"
   do
      printf "Removing user: $user from group: $group\n"
      aws iam remove-user-from-group --group-name $group --user-name $user --profile $profile
   done
   printf "Deleting group: $group\n"
   aws iam delete-group --group-name $group --profile $profile
done

USERS=($(aws iam list-users --output text --query 'Users[*].UserName' --profile $profile))
for user in "${USERS[@]}"
do
   USERPOLICIES=($(aws iam list-user-policies --output text --query 'PolicyNames[*]' --user-name $user --profile $profile))
   for policy in "${USERPOLICIES[@]}"
   do
      printf "Removing policy: $policy from user: $user\n"
      aws iam delete-user-policy --user-name $user --policy-name $policy --profile $profile
   done
   printf "Deleting user: $user\n"
   aws iam delete-login-profile --user-name $user --profile $profile
   aws iam delete-user --user-name $user --profile $profile
done

if [ -n "$singleRegion" ]
then
   ARRAY=($singleRegion)
else
   # Iterate through each AWS region and remove any labs 
   ARRAY=($(aws ec2 describe-regions --query Regions[*].RegionName --output text --profile $profile))
fi

ARRAY=("us-east-1" "${ARRAY[@]}")

for region in "${ARRAY[@]}"
do

   printf "."
   #printf "$region\n"

   # Tell the aws cli to use a region. The region is whichever region we are currently on in the array above. 
   aws configure set region $region --profile $profile
   if [ $? -ne 0 ]
   then
     printf "Error occured connecting to the account\n"
     printf "Try re-running the script again in a few seconds\n"
     return 1
   fi

   # Get a list of instances running in this region and terminate them.   
   INSTANCES=($(aws ec2 describe-instance-status --query InstanceStatuses[*].InstanceId --output text --profile $profile))
   
   
   INSTANCES_STRING=""
   for i in "${INSTANCES[@]}"
   do
      #Add each instance to a string so that we can terminate each of them in one command below
      INSTANCES_STRING+=" $i"

      # However termination protection will have to be turned off for each instance one by one first
      aws ec2 modify-instance-attribute --no-disable-api-termination --instance-id $i --profile $profile
   done
   
   if [ -n "$INSTANCES_STRING" ]
   then
      printf "\nTerminating Instances in $region"
      aws ec2 terminate-instances --instance-ids $INSTANCES_STRING --profile $profile
      if [ $? -ne 0 ]
      then
        printf "FAILED to terminate all Instances in $region\n"
        printf "Try re-running the script again in a few seconds\n"
        return 1
      fi
   
      # Give instances time to shut down - This should really be implemented to wait until all instances show as terminated but it seems to work fine this way
      sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "."
      sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "."
      sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf "." && sleep 10 && printf ".\n"
   fi

   # Linux SSH keys will be deleted
   KEYS=($(aws ec2 describe-key-pairs --query KeyPairs[*].KeyName --output text --profile $profile))

   for KEY in "${KEYS[@]}"
   do
      printf "\nDeleting ssh keys in $region\n"
      aws ec2 delete-key-pair --key-name $KEY --profile $profile
      if [ $? -ne 0 ]
      then
         printf "$KEY FAILED to Delete in $region\n"
         printf "Try re-running the script again in a few seconds\n"
         return 1
      fi
   done

   
   # Get a list of stacks in the region and delete them. Deleting a stack will deelte all resources created by the stack, Networks, GWs, SecGrps etc. 
   STACKS=($(aws cloudformation describe-stacks --query Stacks[*].StackName --output text --profile $profile))

   #printf "Found the following stacks : \n"
   #for STACK in "${STACKS[@]}"
   #do
   #   echo $STACK
   #done
   
   for STACK in "${STACKS[@]}"
   do


      # The first thing that needs to be removed is any buckets. 
      # If a bucket was created by a stack then the stack will try and remove it
      # This will fail if the bucket is not empty
      # So we will remove the buckets first and then the stacks
      BUCKETS=($(aws s3 ls --profile $profile | awk '{print $3}'))

      for bucket in "${BUCKETS[@]}"
      do
         printf "\nDeleting the bucket: $bucket and its contents\n"
         aws s3 rm s3://$bucket --recursive --profile $profile > /dev/null
         aws s3api delete-bucket --bucket $bucket --profile $profile
         if [ $? -ne 0 ]
         then
            printf "Error occured removing the Bucket $bucket\n"
            printf "Try re-running the script again in a few seconds\n"
            return 1
         fi
      done
   
      printf "\nDeleting stack: $STACK in $region \n"
      aws cloudformation delete-stack --stack-name $STACK --profile $profile
      if [ $? -ne 0 ]
      then
        printf "$STACK FAILED to Delete in $region\n"
        printf "Try re-running the script again in a few seconds\n"
        return 1
      fi
   
      printf "Waiting for $STACK to delete ..."
      cfStat=$(aws cloudformation describe-stacks --stack-name $STACK --profile $profile --query 'Stacks[0].[StackStatus]' --output text)
      while [ "$cfStat" = "DELETE_IN_PROGRESS" ]
      do
        sleep 5
        printf "."
        cfStat=$(aws cloudformation describe-stacks --stack-name $STACK --profile $profile --query 'Stacks[0].[StackStatus]' --output text)
        if [ "$cfStat" = "DELETE_FAILED" ]
        then
          printf "\n$STACK FAILED to delete in $region\n"
          printf "Try re-running the script again in a few seconds\n"
          return 1
        fi
      done
      
      # In the loop above we run the desribe stack command to and pull out the current state. Once the Stack is deleted we will get the error below as it no longer exists. 
      # There is no way to supporess this message unless we supress all output from he stack deletion which would be bad if something else when wrong
      printf "An error occurred (ValidationError) when calling the DescribeStacks operation: Stack with id $STACK does not exist << IGNORE THIS ERROR IF SEEN ABOVE\n"
      printf "\n$STACK deleted in $region\n"


      # Cloudtrail will be deleted here as we know the region with the Stack is also the region with the trail
      TRAILS=($(aws cloudtrail describe-trails --query trailList[*].Name --output text --profile $profile))
      for trail in "${TRAILS[@]}"
      do
         printf "\nDeleting the cloudTrail: $trail\n"
         aws cloudtrail delete-trail --name $trail --output text --profile $profile 
         if [ $? -ne 0 ]
         then
            printf "Error occured removing the CloudTrail: $trail\n"
            printf "Try re-running the script again in a few seconds\n"
            return 1
         fi
      done

      # VPC flows will be deleted here as we know the region with the Stack is also the region with the flow
      FLOWS=($(aws ec2 describe-flow-logs --query FlowLogs[*].FlowLogId --output text  --profile $profile))
      for flow  in "${FLOWS[@]}"
      do
         printf "\nDeleting VPC Flow: $flow\n"
         aws ec2 delete-flow-logs --flow-log-ids $flow  --profile $profile > /dev/null
         if [ $? -ne 0 ]
         then
            printf "Error occured removing the VPC Flow: $flow\n"
            printf "Try re-running the script again in a few seconds\n"
            return 1
         fi
      done
   
      # Delete log groups
      LOGGROUPS=($(aws logs describe-log-groups --query logGroups[*].logGroupName --output text --profile $profile))
  
      for group in "${LOGGROUPS[@]}"
      do
         printf "\nDeleting Log Group: $group\n"
         aws logs delete-log-group --log-group-name $group --output text --profile $profile
         if [ $? -ne 0 ]
         then
            printf "Error occured removing the Log Group: $group\n"
            printf "Try re-running the script again in a few seconds\n"
            return 1
         fi
      done

      BUCKETS=($(aws s3 ls --profile $profile | awk '{print $3}'))
   
      for bucket in "${BUCKETS[@]}"
      do
         printf "\nDeleting the bucket: $bucket and its contents\n"
         aws s3 rm s3://$bucket --recursive --profile $profile > /dev/null
         aws s3api delete-bucket --bucket $bucket --profile $profile
         if [ $? -ne 0 ]
         then
            printf "Error occured removing the Bucket: $bucket\n"
            printf "Try re-running the script again in a few seconds\n"
            return 1
         fi
      done

   done

   # Here we delete all Security Groups except for the default one as it can't be deleted and would cause an error
   SECGROUPS=($(aws ec2 describe-security-groups --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text --profile $profile))

   # Counter used to count how many times we've tried to delete a Security Group
   cfcntr=0
   for secgroup in "${SECGROUPS[@]}"
   do
     if [ $secgroup != "default" ]
      then

         # Sometimes deleting a security group will fail if something refers to it
         # Usually it's a instance that hasn't fully terminated yet
         # If the security group deletion fails we will wait and try angain 10 times
         printf "Deleting the Security Group: $secgroup\n"
         aws ec2 delete-security-group --group-id $secgroup --output text --profile $profile > /dev/null 2>&1
         # Capture any errors as actOut
         actOut=$?
         # If there are errors then wait 5 seconds and try again
         #  We will do this 10 times
         while [[ $actOut -ne 0 && $cfcntr -le 10 ]]
         do
           printf "."
           sleep 30
           aws ec2 delete-security-group --group-id $secgroup --output text --profile $profile > /dev/null
           actOut=$?
           if [ $actOut -eq 0 ]
           then
             break
           fi
           printf "."
           cfcntr=$[$cfcntr +1]
         done
      fi
   done

   # If we tried 10 times and it's still not answering then we try one more time but print the error to the screen and exit if it fails again 
   if [ $cfcntr -gt 10 ]
   then
     aws ec2 delete-security-group --group-id $secgroup --output text --profile $profile
     if [ $? -ne 0 ]
     then
        printf "Error occured removing the Security Group: $secgroup\n"
        printf "Try re-running the script again in a few seconds\n"
        return 1
     fi
   fi
   
   # Here we delete all roles except for the built in ones which can't be deleted and would cause an error
   # When delting a role you also have to detach/delete any policies or profiles it has 
   ROLES=($(aws iam list-roles --query Roles[*].RoleName --profile $profile --output text))
   
   string=""
   for role in "${ROLES[@]}"
   do
      if [ "$role" != "AWSServiceRoleForAmazonGuardDuty" ] && [ "$role" != "AWSServiceRoleForOrganizations" ] && [ "$role" != "OrganizationAccountAccessRole" ] && [[ "$role" != Sensor-ReadOnlyRoleWithCloudTrailManagement* ]] && [ "$role" != "AWSServiceRoleForSupport" ] && [[ "$role" != "AWSServiceRoleForTrustedAdvisor" ]]
      then
        printf "\nDeleting Role: $role\n"
        PROFS=($(aws iam list-instance-profiles-for-role --role-name $role --query InstanceProfiles[*].InstanceProfileName --output text --profile $profile))
        for prof in "${PROFS[@]}"
        do
           aws iam remove-role-from-instance-profile --instance-profile-name $prof --role-name $role --profile $profile
           aws iam delete-instance-profile --instance-profile-name $prof --profile $profile
        done
        ATTACHEDPOLICIES=($(aws iam list-attached-role-policies --query AttachedPolicies[*].PolicyArn --role-name $role --output text --profile $profile))
        for policy in "${ATTACHEDPOLICIES[@]}"
        do
           aws iam detach-role-policy --role-name $role --policy-arn $policy --profile $profile
        done
        ROLEPOLICIES=($(aws iam list-role-policies --role-name $role --query PolicyNames[*] --output text --profile $profile))
	for policy in "${ROLEPOLICIES[@]}"
        do
            aws iam delete-role-policy --role-name $role --policy-name $policy --profile $profile
        done
        aws iam delete-role --role-name $role --profile $profile
        echo " "
      fi
   done
done

printf "\nRemoving all references to the profile: $profile from users cron\n"
crontab -l | grep -v "$profile " | crontab -

printf "\n"

return 0

}

retry=0
function main
{
echo "Running removeAccount with $1"
removeAccount $1
if [ $? -ne 0 ] && [ $retry -le 10 ]
then
  retry=$[$retry+1]
  echo "RETRY COUNT: $retry for $1"
  printf "An error occured so I will try again in thirty minutes\n"
  sleep 1800
  main $1
  return
elif [ $retry -gt 10 ]
then
  printf "**************ERROR****************\nUnable to stop $1 after 10 attempts. Manual intervention required\n"
  sendEmail.py -r USM-Anywhere-Training@alienvault.com -s "$1 Failed to Power Off" -b "Do not reply to this e-mail<br/>The labFinish script tried to power off $1 10 times and failed.<br/>Manual intervention is required<br/>"
  return
else
  printf "Done\n"
fi
}

for account in "${accNameArray[@]}"
do
  echo "Removing account: $account"
  main $account
done
